\section{Problem formulation and mdp model} \label{sec:model}
%Microgrids comprise of the distributed small scale power generating sources, mainly renewable energy sources (mainly wind and solar ) and  are also equipped with storage devises.
We consider $N$ microgrids denoted by $\{1,\ldots,N\}$, which are inter-connected through the central electric grid distribution network. Each microgrid comprises of the distributed small scale renewable power generation sources that are equipped with energy storage devices. Let $B_i$ be the maximum energy storage capacity of microgrid $i$. At every time step $t$ of a day, the $i^{th}$ microgrid controller $C_i$ has access to the following information:
\begin{enumerate}[label=(\alph*)]
\item Total energy ($r_t^i$) generated from all it's energy sources.
% (super script $i$ used to refer to microgrid $i$).
\item Accumulated non-ADL demand ($d_t^i$) from each load. 
\item Set of all ADL jobs ($J_{t}^{i}$). $J_{t}^{i}$ has the form $\{\gamma_{1}^{i},\ldots,\gamma_{n}^{i}\}$, where the $j^{th}$ ADL job $\gamma_{j}^{i} = (a_{j}^{i}, f_{j}^{i})$. Here, $a_{j}^{i}$ represents the number of units of energy required to finish the job, and  $f_{j}^{i}$ represents the number of future time slots remaining by which one can schedule the job without incurring a penalty.
\item  Total energy available ($b_{t}^{i}$) in it's storage device.
\end{enumerate} 
%In this paper, we consider the cooperative energy exchange model under which microgrids can share energy among themselves. 
From the above available information, microgrid controller  $C_i$ at every time step $t$ has to decide on the following choices: (a)  Amount of energy it needs to buy (sell) from (to) the main grid, (b) Amount of energy it needs to buy (sell) from (to) the neighboring microgrids,
(c) Amount of energy it needs to store (retrieve) into (from) the storage device, and (d) The subset of ADL jobs it needs to schedule. Both the demand and energy generated at each microgrid is uncertain due to the random nature of loads ($d_t^i$ and $A_t^i$) and the renewable energy generation ($r_t^i$). 

MDP is a general framework for modeling problems of dynamic optimal decision making under uncertainty. An MDP is a tuple $<\mathcal{S},\mathcal{U},\mathcal{R},\mathcal{P}>$, where $\mathcal{S}$ is the set of all states, $\mathcal{U}$ is the set of  feasible actions, $\mathcal{R}:\mathcal{S}\times\mathcal{U}\times\mathcal{S}\to \mathbb{R} $ is the single- stage reward function and $\mathcal{P}$ is the transition probability matrix. In RL, an agent interacts with the environment by observing state $s_t \in \mathcal{S}$ and  making decisions $u_t \in \mathcal{U}$. The new state $s_{t+1}$ is obtained from the transition probability $ \tilde p(s_{t+1} | s_t,u_t)$ and yielding a reward $r_{t+1} = \mathcal{R}(s_t,u_t,s_{t+1})$. The goal of  RL agent is to learn the optimal sequence $\{u_t\}$ of actions so as to maximize its total expected return.
% by selecting actions $u_t$ at every time instant. We assume that actions are taken from the policy $\pi : \mathcal{S} \times \mathcal{U} \to [0,1]$. $\pi(u_t | s_t)$ denotes the probability of selecting action $u_t$ when agent is in state $s_t$. 
%We modeled our problem in the framework of MDP. 
 In the next subsection we provide the details of our MDP model.
\subsection{MDP framework}
We begin by specifying the states, actions and single-stage rewards, for the MDP model.
\subsubsection{State space}
The state $s_{t}^{i}$ at time instant $t$  for the microgrid $i$ is as follows:
\begin{align}
s_{t}^{i} = (t,nd_{t}^{i},p_{t}, J_{t}^{i}),
\end{align}
where the net demand $nd_{t}^{i} = r_{t}^{i} + b_{t}^{i} - d_{t}^{i}$.
%, signifies excess power or deficit depending on whether $nd_{t}^{i} > 0$ or $nd_{t}^{i} < 0$.
If $nd_{t}^{i} > 0$, then  there is excess of power after meeting the non-ADL demand and if $nd_{t}^{i} < 0$, there is a deficit in power even to meet the non-ADL demand. Also $p_{t}$ denotes the price per unit energy. The state also includes time $t$ since optimal action can depend on it. For example, a microgrid operating on solar renewable generation can sell excess power during the morning as the solar power will be available even during afternoon. But it may not be a good choice to sell it in the evening as there will be no solar power during the night. 
\subsubsection{Action space}
At each time instant $t$, the microgrid controller needs to make two decisions $u_{t}^{i}$ and $v_{t}^{i}$. The first action $u_{t}^{i}$, if positive, denotes the number of units that the microgrid is willing to sell and if negative, represents the number of units that the microgrid is willing to buy. The second action $v_{t}^{i}$ pertains to the scheduling decision of ADL jobs taken by microgrid $i$.

Let $P_{t}^{i}$ be the power set of $J_{t}^{i}$, which consists of all possible combinations of the ADL jobs that can be scheduled at time instant $t$ at microgrid $i$. Let  $A_{t}^{i}$ be a set, where each element denotes the total aggregated ADL demand of an element in $P_{t}^{i}$. For example, $j^{th}$ element $A_{t}^{i}(j) = \sum_{k=1, \gamma_k^i \in P_{t}^{i}(j) }^n a_k^i$, where $ P_{t}^{i}(j)$ is the $j^{th}$ element in  $P_{t}^{i}$ and $n$ is the total number of elements in $P_{t}^{i}$.
%Let $P_{t}^{i} = \{\Gamma_{1}^{i},\ldots,\Gamma_{N}^{i}\}$ be the power set of $J_{t}^{i}$, which consists of all possible combinations of the ADL jobs that can be sheduled at time instant $t$ at microgrid $i$. 
%Let  $A_{t}^{i} = \{A(\Gamma_{1}^{i}),\ldots,A(\Gamma_{N}^{i})\} $, where $A(\Gamma_{j}^{i}) = \sum_{\gamma_{k}^{i} \in \Gamma_{j}^{i} } a_{k}^{i}$.
 The feasible region for action $u_{t}^{i}$ is bounded as follows:
\begin{align}
-min(M_t^i, B_t^i - nd_t^i + &\max_{1\leq j \leq 2^n} A_t^i(j) ) \leq u_t^i \nonumber\\ &\leq max(0, nd_t^i),
\end{align}
where $M_t^i$ denotes the maximum amount of power the main grid can give to microgrid $i$. This constraint is to maintain stability of the main grid. The above bounds indicate that the  microgrids can sell energy if there is surplus; or can buy energy either to meet the non-ADL demand, or to store in the battery, or to meet ADL demand. The energy bought is either stored or used to meet the ADL demand only after satisfying the non-ADL demand. 
%The intuition behind the bounds is as follows. A microgrid can sell atmost the excess power. That is, the power remaining after meeting the demand. While buying, it can buy to meet the demand and also to fill its battery.

After the controller picks action $u_{t}^{i}$, we construct the feasible set $F_{t}^{i}$ (subset of $P_{t}^{i}$) which  consists of all possible subsets of ADL jobs that can be scheduled with $u_{t}^{i}$. More formally, each element $j$ of  $F_{t}^{i}$ has to satisfy the following condition:   $A_t^i(j) \leq u_{t}^{i} $, where $A_t^i(j)$ is the total energy required to finish all the ADL jobs in it. The controller picks action $v_{t}^{i}$ which is an element of $F_{t}^{i}$, which results in scheduling all the ADL jobs in that subset. The remaining power is used to meet the non-ADL demand or for storage in the battery.

%Each element $\Gamma_{j}^{i}$ in the $F_{t}^{i}$ has to satisfy the following condition $A(\Gamma_{j}^{i}) \leq u_{t}^{i} $.
%Agent has to pick the action $v_{t}^{i} = \Gamma_{j}^{i} \in F_{t}^{i}$. Now ADL jobs in $\Gamma_{j}^{i}$ will get sheduled. 
 Let $\widehat J_{t+1}^{i}$ be the new set of ADL jobs received by controller at time instant $t+1$. Depending on action $v_{t}^{i}$, some of the ADL jobs will not get scheduled. These are then considered in time step $t+1$, if they can be scheduled without incurring any penalty. The set of all ADL jobs at time instant $t+1$ is the union of the new and old ADL jobs which are not scheduled even after reducing $f_{j}^{i}$ by one (number of future time slots remaining by which one can schedule that job without incurring penalty).
%For the next time instant $t+1$, we update the following :
We have $J_{t+1}^{i} = \widehat J_{t+1}^{i} \cup \widetilde J_{t}^{i}$, where $\widetilde J_{t}^{i} =  \{(a_{1}^{i}, f_{1}^{i} - 1),\ldots,(a_{n}^{i}, f_{n}^{i} - 1)\}$, and $ (a_{j}^{i}, f_{j}^{i}) \in \overline J_{t}^{i}$. Further, $\overline J_{t}^{i} = J_{t}^{i} - v_{t}^{i}$.

The battery information is updated as follows:
\begin{align}
b_{t+1}^{i} = max(0,nd_{t}^{i} - u_{t}^{i} + v_{t}^{i}),
\end{align}
which denotes the power available after meeting the non-ADL demand and after meeting part of the ADL demand.
\subsubsection{Single-stage reward function}
We want to maximize the profit of each microgrid obtained by selling power while reducing the demand and supply deficit. Our single-stage reward function has both the reward obtained by selling power and penalty for unmet demand. The single-stage reward  function for our MDP is as follows :
\begin{align}
r^{i}(s_t^i,u_t^i) = p_{t}*u_{t}^{i} + c*min(0,&nd_{t}^{i} - u_{t}^{i})  \nonumber\\ & - c* \sum_{k =1}^{n} I_{\{f_{k}^{i} = 0\}} a_{k}^{i} .
\end{align}
The first term represents the loss/gain incurred for  buying/selling  power while the second and third terms represent the penalty  incurred for not meeting the non-ADL demand and ADL demand respectively. Here, $c (\ge 0)$ is penalty per unit of unmet demand and $I_{\{f_{k}^{i} = 0\}}$ is the indicator random variable which is equal to one if $f_{k}^{i} =0$ and zero otherwise. 
%The microgrid incurs a cost of $c$ for every unit of demand that is not met. 
%******Need to write about transition probability kernel ***********.
Next, we provide the long-run average cost objective function. 
\subsection{Average cost setting} \label{subsec:avg}
The long-run average cost objective function $J(\pi)$ of the microgrid $i$ for a given policy $\pi$ is given as follows:
%\cite{avgcost}:
\begin{align}
J(\pi) := \lim \sup_{n \rightarrow \infty}\frac{1}{n} \,\, \E \left(\left.\sum_{k = 0}^{n} r^{i} (s_{k},u_{k})\right|\pi \right),
\end{align}
where $E(.)$ denotes the expected value. Here we view a policy $\pi$ as the map $\pi : S \to A$ which assigns for any state $s$, a feasible action $a$. The goal of our RL agent is to find $\pi^* = \argmax_{\pi \in \Pi} J(\pi)$, where $\Pi$ is the set of all feasible policies.

%We also consider the long run discounted cost formulation. The objective here is to maximize the following:
%
%\begin{align}
%limsup_{n \rightarrow \infty} \sum_{t = 0}^{n} \gamma^{t} * E(g^{i}(s_{t},u_{t})),
%\end{align}
%
%where $\gamma$ is the discount factor. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\algblock{PEval}{EndPEval}
%\algnewcommand\algorithmicPEval{\textbf{\em Function evaluation 1}}
% \algnewcommand\algorithmicendPEval{}
%\algrenewtext{PEval}[1]{\algorithmicPEval\ #1}
%\algrenewtext{EndPEval}{\algorithmicendPEval}
%
%\algblock{PEvalPrime}{EndPEvalPrime}
%\algnewcommand\algorithmicPEvalPrime{\textbf{\em Function evaluation 2}}
% \algnewcommand\algorithmicendPEvalPrime{}
%\algrenewtext{PEvalPrime}[1]{\algorithmicPEvalPrime\ #1}
%\algrenewtext{EndPEvalPrime}{\algorithmicendPEvalPrime}
%
%\algblock{PImp}{EndPImp}
%\algnewcommand\algorithmicPImp{\textbf{\em Gradient descent}}
% \algnewcommand\algorithmicendPImp{}
%\algrenewtext{PImp}[1]{\algorithmicPImp\ #1}
%\algrenewtext{EndPImp}{\algorithmicendPImp}
%
%\algtext*{EndPEval}
%\algtext*{EndPEvalPrime}
%\algtext*{EndPImp}
%%%%%%%%%%%%%%%%% alg-custom-block %%%%%%%%%%%%
%\algblock{PEvalPrimeDouble}{EndPEvalPrimeDouble}
%\algnewcommand\algorithmicPEvalPrimeDouble{\textbf{\em Function evaluation 3}}
% \algnewcommand\algorithmicendPEvalPrimeDouble{}
%\algrenewtext{PEvalPrimeDouble}[1]{\algorithmicPEvalPrimeDouble\ #1}
%\algrenewtext{EndPEvalPrimeDouble}{\algorithmicendPEvalPrimeDouble}
%\algtext*{EndPEvalPrimeDouble}
%
%\algblock{PImpNewton}{EndPImpNewton}
%\algnewcommand\algorithmicPImpNewton{\textbf{\em Newton step}}
% \algnewcommand\algorithmicendPImpNewton{}
%\algrenewtext{PImpNewton}[1]{\algorithmicPImpNewton\ #1}
%\algrenewtext{EndPImpNewton}{\algorithmicendPImpNewton}
%
%\algtext*{EndPImpNewton}
%
%%%%%%%%%%%%%%%%%%%%
%
%\begin{algorithm}[t]
%\begin{algorithmic}
%\State {\bf Input:} 
%initial parameter $x_0 \in \R^N$, perturbation constants $\delta_n>0$, step-sizes $\{a_n, b_n\}$, operator $\Upsilon$.
%\For{$n = 0,1,2,\ldots$}	
%	\State Generate $\{d_n^{i}, i=1,\ldots,N\}$, independent of $\{d_m, m=0,1,\ldots,n-1\}$. 
%	\State For any $i=1,\ldots,N$, $d_n^{i}$ is distributed either as an asymmetric Bernoulli (see \eqref{eq:det-proj}) or Uniform $U[-\eta,\eta]$ for some $\eta >0$ (see Remark \ref{remark:unif}). 
%	\PEval
%	    \State Obtain $y_n^+ = f(x_n+\delta_n d_n) + \xi_n^+$.
%  \EndPEval
%	    \PEvalPrime
%	    \State Obtain $y_n^- = f(x_n-\delta_n d_n) + \xi_n^-$.
%	    \EndPEvalPrime
%	    	    \PEvalPrimeDouble
%	    \State Obtain $y_n = f(x_n) + \xi_n$.
%	    \EndPEvalPrimeDouble
%	    \PImpNewton
%		\State Update the parameter and Hessian as follows:
%		\begin{align*}
%		x_{n+1} = & \; x_n - a_n \Upsilon(\overline H_n)^{-1}\widehat\nabla f(x_n), \\
%\overline H_n = &\; (1-b_{n})  \overline H_{n-1} + b_{n} ( \widehat H_n - \widehat \Psi_n),
%\end{align*}
%where $\widehat H_n$ and $\widehat \Psi_n$ are chosen according to \eqref{eq:2rdsa-estimate-ber} and \eqref{eq:psinhat}, respectively. 
%		\EndPImpNewton
%\EndFor
%\State {\bf Return} $x_n.$
%\end{algorithmic}
%\caption{Structure of 2RDSA-IH algorithm.}
%\label{alg:structure}
%\end{algorithm}
