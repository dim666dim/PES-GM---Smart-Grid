\begin{abstract}
This paper considers two important problems - on the supply-side and demand-side respectively and studies both in a unified framework. On the supply side, we study the problem of energy sharing among microgrids with the goal of maximizing profit obtained from selling power while meeting customer demand. On the other hand, under shortage of power, this problem becomes one of deciding the amount of power to be bought with dynamically varying prices. On the demand side, we consider the problem of optimally scheduling the time-adjustable demand - i.e., of jobs with flexible time windows in which they can be scheduled. While previous works have treated these two problems in isolation, we combine these problems together and provide for the first time in the literature, a unified Markov decision process (MDP) framework for these problems. We apply the Q-learning algorithm, a popular model-free reinforcement learning technique, to obtain the optimal policy. Through simulations, we show that our model outperforms the traditional power sharing models.



%In this paper, we consider two problems, one on the supply-side management another on the demand-side management of the smartgrid.
%%energy sharing among microgrids along with optimal scheduling of time adjustable smart home appliances demand. 
%On the supply-side, we study the problem of energy sharing among the microgrids with the goal of maximizing the profit obtained by selling the power while meeting the demand of its customers. Microgrids are equipped with a limited capacity batteries that can store the renewable energy. When they have excess power after meeting its demand, they need to take a decision on the amount of power to be sold and that to be stored in their batteries. This is a crucial decision to make as the renewable energy generation is uncertain in nature and the demand of its customers varies from time to time. A penalty is levied when the microgrid cannot satisfy the demand in any time instant. Similarly when there is a shortage of power, a decision needs to be taken on the amount of power to be bought. This decision is influenced by varying price of the power. Hence there is a control problem of taking the optimal decision on buying/selling the power in order to maximize its long term average profit.  
%
%On the demand-side, we consider the problem of optimally scheduling the time adjustable demand. These are the jobs that have a flexible time window in which they can be scheduled. Penalty is levied only if they are not scheduled at the end of their window. This allows us to intelligently schedule them so as to balance the load at the microgrid. For the first time, we combine these problems together and formulate them in a Markov Decision Process (MDP) framework. We apply a popular Reinforcement Learning algorithm Q-learning to provide solution to this problem. Through simulations, we show that our model outperforms the traditional power sharing models. 

 
%We formulate both the problems in a unified Markov decision process (MDP) framework for the first time to incorporate envisioned future smartgrid system.  We considered the infinite horizon average cost as objective function and used Q-learning algorithm at each microgrid. In simulations,
%we observed that our algorithms outperforms all the other algorithms by performing optimal choices for energy sharing and scheduling.






%One of the key challanges of Smart Grid management is ensuring to meet the demand while having control on the supply cost. 
%%\st{This concept of Smart Grid has become popular in the recent times. The main objective in this Smart Grid is to intelligently make use of power.} 
% It involves  performing optimal actions both at the production and consumption sites of  electric grid. In this work, we consider the problem of managing multiple microgrids attached to a central smart grid. Each of these microgrids are equipped with batteries to store renewable power.
% %\st{In this work, we consider multiple microgrids equipped with batteries to store renewable power}.
% At every instant, each of them receive a demand to meet. Depending on the supply (i.e., currently available battery energy, power drawn either from the central grid or from the peer microgirds), each of the microgrids take a decision on from where to draw the energy to meet the demand.
%% \st{Depending on the current battery and renewable power information, they take a decision on number of power units to be bought or sold.}
% When a microgrids buys energy either from the central grid or from the peer microgrids, it can use that energy either to meet the current demand or to store in the battery storage for future use. 
%%\st{If any of the other neighboring microgrids sell the power, they can consume it. Otherwise, they can get it from the main grid. If power is bought, it is first used to meet its demand and rest of it will be stored in the battery.}
% Hence, there is a control decision problem at each microgrid on the number of power units to be bought or sold at every time instant. We note that both the forecasted demand and predicted renewable supply impact this decision by each microgrid. 
%%\st{We note that the future forecast demand and renewable units also impact this decision.}
% Further, we consider some amount of the forecasted demand to be adjustable in terms of the time when it can be met by the microgrid. Such an adjustable demand is attributed due to the activities of daily living pertraining to Smart Homes connected to the microgrid networks. Hence, we formulate this problem in the framework of Markov Decision Process and apply Reinforcement Learning algorithms to solve this problem. Through simulations, we show that the policy we obtain performs an significant improvement over traditional techniques.
\end{abstract}