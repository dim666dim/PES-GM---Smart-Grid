\section{Algorithm}\label{sec:algo}

%We first note that the renewable generation is uncertain in nature. 
%That is, we do not know in the current time period, the renewable generation in the future time periods.

 In this paper, we do not assume any model of the system (probability transition model of the demand, supply and reward structure) due to uncertainity of renewable energy generation. We employ RL agorithms which does not assume any model to provide optimal solution.

We employe Q-Learning algorithm, a  popular RL method for solving the average cost probelm in \ref{subsec:avg}.
%To solve the above average cost problem, we apply a popular RL algorithm, Q-Learning.
 Our objective is to obtain a stationary optimal policy $\pi : S \rightarrow A$, which is a mapping from state to the action space.  
We apply the Relative Value Iteration (RVI) based Q-Learning described in \cite{avgcost}. In this algorithm, we update the Q-values in each iteration according to the following rule:
\begin{align}
Q^{n+1}(&s,u) = Q^{n}(s,u) + \alpha(n)(g(s,u,s^{'}) + \nonumber\\ &  max_{u} Q^{n}(s^{'},u) - max_{u} Q^{n}(s_{0},u) - Q^{n}(s,u)),
\end{align}
where $\alpha$ is the learning rate and $s_{0}$ is any prescribed state.
The $Q^n(s,u)$ represents the $n$th estimate of the Q-value  obtained in state $s$ by taking an action $u$. In \cite{avgcost}, it is shown that under appropriate learning rate, the algorithm converges to the  optimal policy. 
Each microgrid will run the algorithm independently until convergence. Then the optimal policy of microgrid $i$ is obtained as follows
\begin{align}
\pi_{i}^{*}(s) = max_{u}Q^{i}(s,u),
\end{align}
that is, the optimal policy in state $s$ is selected by taking the maximum over all actions of the Q-values.  

